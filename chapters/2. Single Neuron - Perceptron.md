# 2.1 Sample a problem to solve

Let’s say you own a sailboat.  
You gather the daily, often publicly available, information:  
- wind speed (knots, or m/s)  
- wave height (in meters because you are a scientist)  
- ambient temperature (°C)  
- wind direction (° true north)  
- small-craft advisory (true or false)  
- experienced crew (0 for single-handled, 1 for single mate, 2 for two mates, etc.)  
Each sail (or sail “leg” if conditions change) you record whether sailing conditions were enjoyable on the 0 to 10 (terrible to nirvana) scale.  
After analyzing the “captain’s logs” of multiple years, you have a spreadsheet of about 10 columns and hundreds of rows. Adjust the data to your own needs and interests.

# 2.2 Linear Math Equation

Because we want to start with a simple linear model,  we have to oversimplify few assumptions:

  

- for now let’s **ignore** annoying no wind situations, 
- enjoyment decrease linearly as wind starts blowing hard,
- enjoyment decrease linearly as wave height increase,
- enjoyment increase with temperature, 
- for now let’s **ignore** scorching heat,
- enjoyment increases with crew experience.

  

A **linear model** math looks like this:
$$
y=w1*x1 +w2*x2 +w3*x3 +w4*x4 +w5*x5 +w6*x6 +b
$$
or in short:
$$
y = \sum_i w_i x_i + b
$$
  

where:

- y: is **enjoyment** calculated,
- Σ: sigma means sum of all multiplications
- x: are your **input** **features** (wind speed, wave height, etc.),
- w: are the **weights** representing positive or negative influence, 
- b is a **bias** term that shifts the overall output up or down.

  

If the resulting value y is positive, you might interpret it as “enjoyable sailing.” If it’s negative, “maybe stay home”, Of course with positive **bias** set to “a bad day sailing is better than a day spent doing house chores”.

  

**Perceptron as a Linear Function**

A single perceptron can do exactly the computation we need. It takes the dot product of the input vector and the weight vector, adds a bias, and passes the result through an activation function. 

Mathematically:
$$
\hat{y} = f( w_1∗x_1 + w_2∗x_2 + w_3∗x_3 + w_4∗x_4 + w_5∗x_5 + w_6∗x_6 + b )
$$
Weights w and inputs x can be represented as two arrays  
e.g. 

- Array X = [wind, wave, temperature, direction, advisory, crew]
- Array W = [-1.1, -1.1, 1.1, 0, 1, 1.8]

$$
\hat{y} = f( W^{\mathsf{T}}X + b )
$$
The notation   is the **dot product** of the 2 arrays, the weighted sum of inputs

The bias, as the name implies adjust the resulting product of weights and input values to more or less positive direction.

  

That’s all a perceptron does: it learns these numbers automatically from examples so that the equation best matches your recorded outcomes.

  

**Training the PerceptronSingle Neuron - Perceptron**

**Find a problem to solve**

Let’s say you own a sailboat.  
You gather the daily, often publicly available, information:  
- wind speed (knots, or m/s)  
- wave height (in meters because you are a scientist)  
- ambient temperature (°C)  
- wind direction (° true north)  
- small-craft advisory (true or false)  
- experienced crew (0 for single-handled, 1 for single mate, 2 for two mates, etc.)  
Each sail (or sail “leg” if conditions change) you record whether sailing conditions were enjoyable on the 0 (terrible) to 10 (nirvana) scale.  
After analyzing the “captain’s logs” of multiple years, you have a spreadsheet of about 10 columns and hundreds of rows. Adjust the data to your own needs and interests.

**Linear Math Equation**

Because we want to start with a simple linear model,  we have to oversimplify few assumptions:

  

- for now let’s **ignore** annoying no wind situations, 
- enjoyment decrease linearly as wind starts blowing hard,
- enjoyment decrease linearly as wave height increase,
- enjoyment increase with temperature, 
- for now let’s ignore scorching heat waves, after all we are in PNW,
- enjoyment increase with crew experience.

  

A **linear model** math looks like this:

or in short:

  

where:

- y: is **enjoyment** calculated,
- x: are your **input** **features** (wind speed, wave height, etc.),
- w: are the **weights** representing positive or negative influence, 
- b is a **bias** term that shifts the overall output up or down.

  

If the resulting value y is positive, you might interpret it as “good sailing.” If it’s negative, “not good.” That simple boundary — a single straight cut through multidimensional data — is what the perceptron learns.

  

**Perceptron as a Linear Function**

A single perceptron does exactly the computation above. It takes the dot product of the input vector and the weight vector, adds a bias, and passes the result through an activation function. 

Mathematically:

Weights w and inputs x can be represented as two arrays  
e.g. 

- Array X = [wind, wave, temperature, direction, advisory, crew]
- Array W = [-1.1, -1.1, 1.1, 0, 1, 1.8]

  
The notation   is the **dot product** of the 2 arrays, the weighted sum of inputs

The bias, as the name implies adjust the resulting product of weights and input values to more or less positive direction.

  

That’s all a perceptron does: it learns these numbers automatically from examples so that the equation best matches your recorded outcomes.

  

**Training the Perceptron**

You begin with random weights and bias. For each day in your data, the perceptron predicts a score
$$
y

^

\hat{y}

y^ . You then measure the error between that prediction and your recorded rating

y

y

y. The perceptron adjusts its weights slightly in the direction that reduces this error, repeating the process until the model stabilizes.

In PyTorch, the linear model that performs this operation is torch.nn.Linear(input_size, 1). Its forward computation is precisely the dot product and bias addition.

  

import torch

  

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

  

# Suppose we use six measurable inputs

```python

model = torch.nn.Linear(6, 1).to(device)

# Example input: [wind, wave, temp, direction, advisory, crew]
x = torch.tensor([[5.0, 1.2, 17.0, 180.0, 0.0, 1.0]], device=device)

# Predicted sailing enjoyment score (before activation)
z = model(x)

print("Raw perceptron output:", z.item())

This output is the unbounded linear combination of your variables. To convert it into a probability between 0 and 1, pass it through a sigmoid:

prob = torch.sigmoid(z)
print("Probability of enjoyable sail:", prob.item())
```

If that probability is greater than 0.5, the perceptron predicts that conditions will likely be enjoyable.

  

**Understanding the Geometry**

The beauty of this model is its simplicity. Each input adds one dimension to the space, and the perceptron carves that space with a flat surface. The weights are the slope, the bias is the offset, and the activation function is a threshold. It is the mathematical equivalent of your intuition: “If it’s too windy or too cold, the trip won’t be fun.”

When the data truly behave linearly — when the effect of each variable adds up without interaction — a single perceptron can capture the pattern perfectly. It is the foundation for all larger networks. Every layer in a deep model is just a collection of perceptrons stacked together, learning more complex combinations of the same basic rule.

  

**The Next Step**

Of course, not all patterns are linear. You might find that the best sailing days are those with _moderate_ wind — neither too high nor too low — which a single perceptron cannot express. In that case, we will need to combine several perceptrons to bend the decision boundary into a curve. That’s where multilayer perceptrons begin — but every layer still builds on the same linear foundation we just described.