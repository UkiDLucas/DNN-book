# 4.1 Overview 

At the heart of every neural network lies a single idea: data represented as tensors. A tensor is a generalization of scalars, vectors, and matrices into any number of dimensions. If a scalar is a single number, a vector is a one-dimensional array of numbers, and a matrix is two-dimensional, then a tensor is an n-dimensional structure capable of describing almost any kind of data. PyTorch uses tensors as its fundamental data type, allowing them to move seamlessly between CPU and GPU memory through Metal on macOS.

Tensors store numerical values, but their power comes from how efficiently they can be manipulated in bulk. When you perform an operation such as addition, multiplication, or a matrix dot product, the computation runs in parallel across thousands of GPU threads. This parallelism makes deep learning possible, because neural networks depend on millions of such operations repeated continuously during training.

  

# 4.2 Creating Tensors

The simplest way to create a tensor in PyTorch is by using the torch.tensor() constructor. You can also generate random values or fill tensors with zeros and ones. Each tensor has a _shape_ (its dimensions) and a _dtype_ (its numerical precision).

  

import torch

device = torch.device("mps")

x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)

y = torch.ones((2, 2), device=device)

z = torch.rand((2, 2), device=device)

  

Each of these resides in the unified memory shared by the CPU and GPU. On Apple Silicon, this means no explicit transfer of data is necessary, since the memory space is common. You can verify where a tensor is stored by printing x.device.

Because tensors are the building blocks of every model, understanding how to create and shape them is essential. Reshaping operations such as view, reshape, and permute let you organize data in ways that match the layer expectations of a network.

4.3 Tensor Operations

Most neural network layers are compositions of basic tensor operations: addition, multiplication, dot products, and broadcasting. Broadcasting allows PyTorch to automatically expand dimensions when operations involve tensors of different shapes, provided their dimensions are compatible.

  

a = torch.arange(6, device=device).reshape(2,3)

b = torch.tensor([[10],[20]], device=device)

c = a + b

  

In this example, PyTorch automatically broadcasts b across the columns of a. Such automatic expansion eliminates the need for explicit loops and keeps the code concise and efficient. When running on Metal, these operations are fused and parallelized to maximize GPU throughput.

Tensors also support mathematical functions such as torch.sin, torch.exp, and torch.log, all of which can operate elementwise on GPU memory. This allows even complex data transformations to execute in milliseconds.

$.3 Gradients and Autograd

Deep learning depends on gradients — the rates of change of a function with respect to its parameters. PyTorch provides automatic differentiation through a system called _autograd_. When you set a tensor’s requires_grad flag to True, PyTorch begins tracking all operations performed on it. Later, when you call .backward(), it computes derivatives for every parameter that contributed to the final result.

  

x = torch.tensor([2.0, 3.0], device=device, requires_grad=True)

y = x.pow(2).sum()

y.backward()

print(x.grad)

  

The printed gradient corresponds to [4.0, 6.0], the derivative of x² with respect to each element. This process, called _backpropagation_, is what allows neural networks to learn. The Metal backend handles these gradient computations efficiently, using GPU acceleration for both forward and backward passes.
