# 3.1 Overview

There has never been a better time to start deep learning on macOS. Apple’s transition to Apple Silicon changed the landscape completely. The M1, M2, and M3 chips integrate CPU, GPU, and Neural Engine into a single architecture that shares **unified memory**. This eliminates the bottleneck of copying tensors back and forth between the processor and the graphics card. When training a model on Metal, the CPU and GPU simply operate on the same memory space, which makes everything faster and more efficient.

A Mac mini with an M1 processor and 64 GB of unified memory costs around one thousand dollars. Matching that performance with an NVIDIA-based workstation would require a far higher investment, not to mention the noise, power draw, and driver maintenance such systems usually demand. For students, researchers, and independent developers, a modern Mac represents a compact and silent deep learning workstation that can handle both experimentation and production code. The Metal Performance Shaders (MPS) backend in PyTorch unlocks GPU acceleration on macOS without any additional drivers or CUDA installation, making it ideal for this book’s examples.

  

# 3.2 Setup on MacOS**

# 3.3 Homebrew and Python

Although macOS comes with a system version of Python, it is best to install and manage your own development tools to avoid dependency conflicts. The Homebrew package manager simplifies this process. Open Terminal and install it using the command provided on the Homebrew website. Homebrew installs under /opt/homebrew on Apple Silicon and manages software without interfering with the system files.

After installing Homebrew, update it and then install Python and Git. You will use Git for version control and to clone example projects from this book’s repository. Installing Python this way gives you the latest version while keeping the system Python intact.  
Installation steps: 
```z-shell

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
% brew update
% brew --version
Homebrew 4.6.18

% python3 —version
```

At this point you can run Python scripts directly or through an editor such as Visual Studio Code, Xcode, or PyCharm.

# 3.4 Python and Conda Environment

For serious machine learning work, it is essential to isolate dependencies. You can use Python’s built-in venv module or a Conda environment. Conda is especially convenient because it manages binary packages and avoids many compilation issues when dealing with large frameworks like PyTorch.

Navigate and login (Google account) to:

https://www.anaconda.com/download/success

Download Anaconda for Apple Silicon and install as normal package on macOS.

To create an environment using Conda, type 

conda create -n torch_metal python=3.10

Activate it with conda activate torch_metal. Alternatively, you can use the standard virtual environment:  
python3 -m venv venv followed by source venv/bin/activate.

Once the environment is active, all subsequent installations will remain isolated from the system. This protects your setup when new versions of libraries are released and ensures reproducibility for every project in this book. Keep a simple structure under your home folder, such as ~/torch_to_the_metal/notebooks, ~/torch_to_the_metal/data, and ~/torch_to_the_metal/models. It helps maintain order as you work through multiple examples.

# 3.5 Getting Torch

Installing PyTorch on macOS is now simple. Use the command  

pip install torch torchvision torchaudio

  
The latest PyTorch distributions automatically detect Apple Silicon and enable the Metal backend. To verify this, open a Python shell and run:

  
```python

import torch

print(torch.backends.mps.is_available())
```
```
  

If it returns True, the GPU is active. You can then move tensors and models to the Metal device by specifying device = torch.device("mps"). To test this further, run a short script:

  
```
```python
import torch
x = torch.rand(100, 1, device="mps")
y = 3 * x + 2 + 0.1 * torch.randn(100, 1, device="mps")
model = torch.nn.Linear(1, 1).to("mps")
loss_fn = torch.nn.MSELoss()
opt = torch.optim.SGD(model.parameters(), lr=0.1)

  

for _ in range(200):
    opt.zero_grad()
    pred = model(x)
    loss = loss_fn(pred, y)
    loss.backward()
    opt.step()

print(model.weight.item(), model.bias.item())
```

If the printed weight and bias values are close to 3 and 2, your setup is perfect. The computation ran on Metal, demonstrating that gradient propagation and GPU acceleration are working correctly. You can monitor GPU activity through the Activity Monitor app under the GPU tab.

Keep your environment consistent by exporting a requirements file with 

pip freeze > requirements.txt. 

This lets you restore the exact versions later or on another machine. The combination of macOS, PyTorch, and Metal offers a clean, driver-free workflow for experimenting with neural networks at full speed. From here, every subsequent chapter will build upon this foundation, beginning with the most fundamental concept of all — tensors.
# 3.6 Verifying Metal Acceleration Performance

After installing PyTorch and confirming that `torch.backends.mps.is_available()` returns `True`, it is useful to measure how much faster GPU-accelerated Metal (MPS) computations are compared to CPU. A quick benchmark ensures your setup is correctly offloading heavy tensor operations to the GPU.

```python
import torch, time

device_cpu = torch.device("cpu")
device_mps = torch.device("mps")

def benchmark(device):
    x = torch.rand((5000, 5000), device=device)
    y = torch.rand((5000, 5000), device=device)
    if device.type == "mps":
        torch.mps.synchronize()
    start = time.time()
    z = x @ y
    if device.type == "mps":
        torch.mps.synchronize()
    return time.time() - start

t_cpu = benchmark(device_cpu)
t_mps = benchmark(device_mps)

print(f"CPU time: {t_cpu:.3f} s  |  Metal (MPS) time: {t_mps:.3f} s")
# expected: Torch_acceleration.py"
# macOS M1 64Gb RAM: CPU time: 0.128 s  |  Metal (MPS) time: 0.176 s
# gaming ubuntu: CPU time: 0.333 s  |  GPU time: 0.039 s
```

You should see the Metal computation finish several times faster than the CPU.  
Actual performance depends on your Mac model and thermal conditions. If results are similar, verify that:

- Tensors are created directly on the MPS device.  
- Background applications are minimized.  
- macOS and PyTorch are up to date.

This verification ensures that all subsequent chapters — especially those involving training loops — use the full performance of Apple’s unified memory and Metal backend.

# 3.7 macOS vs Ubuntu GPU

Running the full benchmark produced these median times:

| Platform | Device | MatMul (s) | Train Step (s) | Relative Speed |
|-----------|---------|-------------|----------------|----------------|
| Ubuntu (gaming GPU) | CPU | 1.378 | 0.237 | — |
| Ubuntu (CUDA GPU) | GPU | 0.141 | 0.021 | **≈11× faster** |
| macOS M1 (64 GB) | CPU | 0.441 | 0.095 | — |
| macOS M1 (MPS GPU) | GPU | 0.132 | 0.015 | **≈6× faster** |

**Interpretation**

1. On small workloads, CPU BLAS libraries can appear efficient, but GPUs show their advantage once the computation is repeated or chained.  
2. Metal (MPS) performance on Apple Silicon is now in the same order of magnitude as CUDA on high-end NVIDIA hardware for moderate tensor sizes.  
3. Unified memory on M1 eliminates host-device copies, making small-batch operations particularly efficient compared with discrete GPU setups.

**Observation**

- macOS M1’s GPU achieves roughly *6× acceleration* on realistic training steps — an excellent result for a fanless, compact system.  
- Gaming-class CUDA GPUs still lead for large-scale models, but MPS offers remarkable efficiency per watt and unmatched simplicity of setup.

These numbers validate the choice of macOS as a fully capable deep-learning environment for educational and mid-scale research projects.

